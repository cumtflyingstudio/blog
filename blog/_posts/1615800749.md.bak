## 前提
- IDE(Pycharm)
- Fiddler
- Postman
- EDGE or Chrome

使用到的插件

- Requests
- BS4
- Re

现在Cumt的大部分网站都改成前后端分离的模式了，当然还有一堆使用JSP的。。。
技术栈目测是SSM
本文贴出的代码更新于2021/3/8
并不保证以后可以 正常运行
 爬虫部分[在此](https://github.com/boopo/NewCumtLogin)
 web部分[在此](https://github.com/boopo/cumt-kxz)


## 思路
![](https://cdn.nlark.com/yuque/0/2021/jpeg/12616770/1615194713387-14195040-1031-4f98-9a3d-de6c20af6496.jpeg)### 融合门户
#### 提交表单
按下F12 监控一下网络请求，然后点击登录提交表单
找到此请求并分析
![image.png](https://cdn.nlark.com/yuque/0/2021/png/12616770/1615195064756-ee5058bf-7c81-40e4-b187-460322eebf0e.png#align=left&display=inline&height=48&margin=%5Bobject%20Object%5D&name=image.png&originHeight=48&originWidth=450&size=3106&status=done&style=none&width=450)
这是一个Post请求
看一下发送表单

```json
username: 08193109
password: W+ZHWyvdQHDMO1pH/eVV62T6/jYenynl35QxlL8MkgHJC/LP949IzlVyIv558=
captcha: 
_eventId: submit
cllt: userNameLogin
lt: 
execution: 6cf264f8-5ae3-4da3-9d65-b4a33f759676_ZXlKaGJHY2lPaUpJVXpVeE1pSjkuQUJQMmNsZE44dFZLU0xDNjhOTHBFbEI4UTVaWEQ1SUJubXQ2ckJySDNnUmppYVMrRXlZbE1SWmU0VWV3TktmQWlSUC9zUHlZbVFkbkdnZTI4S2g5RnlyQ2NKYXd6QUtJQ0RvczJpNC84emRKZ2V6WHNGMkp5aFdPZmNZQVJkTmZCcTBZZ2hSOGswcnh5SlpYYUczTTdSMVl6RXNxanNjdUVyQm8xaCtabURNSzJWR2pTOEFJZ1JaRHJpOERFZEptakptcWUzamF4dWtGZFJaY011Q2NOTm1rRXRCODNJUnZNd0s1REpaRVlzMTcxc0s2ZE5sQzkqSTY0ZjRRVW81cUtvdnhYd3IyMVlOWVpxdmhIeURLbGgvaVgvRmJ0ZGJvNE8wM1p2WHRHaGFWVnZxWUtSM1E3YlpqZ2VIRjVFblSXpRN1BiQS9ZT1NGKzhjaGF1QjRhd3hKQWdiRFBGS1JTWDZLUzB3L3BKQTN4QlY3TzF5a0wwaHZobzJMSGx1OGd1UFZMdEg1MlppQ1pod3YrenUyQ3NYTE9FcXRxV3cwNnlpelZaYXBxdEkwNlJ1YnlqcVRGamVJdUVtajB2eXlDcS83RnZub3pmcFA5RmVtV0NwRWNyeFVRWFVLbFNURXdOQzh6OVVYUs2emVLa2dqeTBwMUlNZysvRUJZL3oxQUdBZTlPekZWMjdqaGlnOWw4V2hTQVVJUlRPSThjSFVKZUxadXdkU3p5WnNrYjV2SFZsQ1BDK1Z2RzZkRjVCUkZIa1pqQTVyZnRRQXpxMDFRWNsK1Q3cTFadzE4WWRTSFdKKzdkT2ZoaVpucTdyaFgzZU1zNVVEdGNNVGJqVlhzUy9ielZOSTlyNXdoR1h0RGNobFFSbDlPYmNnL01EMXRKcmJRcnJRVEdoa0dtUGc5a1EvdVc1TS9sMmxMcVZKWEJKbnNJWGgzem9nZGJVUWMnFJdTB0VytQR2M1TEJ6dkloTEdzSEtsbCttMk9MdnhrTTlVaFNnWmlxcFV3VFhobllHb0pFWjg1ZzZNQmNoZnNUVlg5YTlVQmlKc0JVZHlwOFVmRm9mVWRpV3F6OERESGJybngyVHh2T1RVOWpHNTg2QTNpdHViYTFuYVlUUFVObExBTVZRcVQ0NGtwdGtjeTFrdWpTZG5HMmhTejlQSFlwZUgwV0VGd081UjBienh5aFVhMk0xbWxZemVlT2RDYnZSUm5HdDhCdkxVQWJnPT0ua21VanNLOXQ2bTBSUnNYSDQtZ3paQkpfXzVQb05qYjBfRXYzRldrM2x6Q1Q5YXVHdGs1a2Q2THpFTzBrcGJzMS1pQ1FfLW1BZl9fcGlRU0hWcGZUamc=
```
有用户名和密码还有别的参数，但是这个密码是加密过的
让我们看一下页面元素
![image.png](https://cdn.nlark.com/yuque/0/2021/png/12616770/1615195451098-c87a0f76-ca9a-4ddb-87c9-87f04471def2.png#align=left&display=inline&height=196&margin=%5Bobject%20Object%5D&name=image.png&originHeight=196&originWidth=872&size=53317&status=done&style=none&width=872)
到这里思路就很清晰了,只需要找到那个加密用的JS函数以及每次的Salt就行了
搜索一下JS pwdEncryptSalt -> encryPassword
然后就找到此函数了
![image.png](https://cdn.nlark.com/yuque/0/2021/png/12616770/1615196043308-e213f964-f33b-4c4c-8240-2e3f61bb55cd.png#align=left&display=inline&height=117&margin=%5Bobject%20Object%5D&name=image.png&originHeight=117&originWidth=419&size=18973&status=done&style=none&width=419)
这个cryptojs是一个前端加密JS库
使用的是AES加密方法
简单介绍一下AES

- 高级加密标准(AES,Advanced Encryption Standard)为最常见的对称加密算法
- 属于对称加密，也就是加密和解密用相同的密钥
- 流程如下

具体的实现还是比较复杂的这里不再叙述
需要注意的一点就是这个密钥是随机生成的，所以我们每次都需要抓取，推荐使用BS4
我这里偷懒使用execjs插件直接运行js函数
```python
def get_token(pwd, salt):
    Passwd = execjs.compile(open(r"aes.js").read()).call('encryptPassword', pwd, salt)
    return Passwd
```
#### 302跳转
一般网站在你点击登录后，为了防止重复提交表单会让你跳转
在这个时候浏览器就不好用了
这时候需要祭出抓包工具 Fiddler
![image.png](https://cdn.nlark.com/yuque/0/2021/png/12616770/1615197036072-7348905b-db02-4386-96f6-c4f5f7cbf859.png#align=left&display=inline&height=202&margin=%5Bobject%20Object%5D&name=image.png&originHeight=202&originWidth=747&size=28963&status=done&style=none&width=747)
你所需要做的就是分析每次需要去哪里跳转，一般都在header里的refer参数或者location或者直接在url里
需要三次请求，因为需要保持会话，所以使用request.session

```python
class newIds:
    def __init__(self, username, password):
        self.username = username
        self.password = password
        self.session = requests.session()

    def login(self):
        r = self.session.get(url=url_login, headers=headers)
        soup = BeautifulSoup(r.text, 'html5lib')
        salt = soup.find('input', id='pwdEncryptSalt')['value']
        execution = soup.find('input', id='execution')['value']
        salt_pwd = get_token(self.password, salt)
        form_login = {
            'username': self.username,
            'password': salt_pwd,
            '_eventId': 'submit',
            'cllt': 'userNameLogin',
            'execution': execution
        }
        rs = self.session.post(url=url_post, data=form_login, headers=headers, allow_redirects=False)

        if rs.status_code == 302:
            url_re = rs.headers['Location']
            rss = self.session.get(url=url_re)
            return True
        else:
            return False
```
#### 能拿到的信息
使用融合门户的cookie，你可以做到如下请求

- 查询校园卡余额
- 查询最近校园卡流水
- 查询图书借阅信息
#### 总结


这样就完成了第一步：登录融合门户
你会收到融合门户的cookie，在request.session.cookies
这里并不涉及持久化，具体请参考矿小助部分
关于多次登录需要验证码的问题，可以使用百度云的文字识别SDK或者直接发给用户，在矿小助中实现了前者
后面的教务系统，图书馆，一卡通三者是并列的，你可以根据需要自行请求
### 教务系统
当你登录用户门户后，一切都好办了，使用request.session会话保持，直接访问教务系统的单点登录url
在经过一些跳转后即可拿到教务系统的JESSIONID
当你获得JESSIONID后，理论上可以请求所有的教务系统后端接口，只需要在header里加上此cookie
经过一些分析，我们拿到了

- 个人信息
- 成绩查询
- 课表查询
- 补考查询
- 考试查询
- 空教室查询

部分代码如下 
以查询课表为例
```python
 class newIds:
    def __init__(self, username, password):
      。。。。
    def login(self):
      。。。。
    def get_jwxt(self):
        headers = {
            'User-Agent': "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:29.0) Gecko/20100101 FireFox / 29.0",
            "X-Requested-With": "XMLHttpRequest"
        }
        r = self.session.get(url=url_jwxt_login2, headers=headers, allow_redirects= False)
        if r.status_code == 302:
            u1 = r.headers['Location']
            r1 = self.session.get(url=u1, headers=headers)
        else:
            return False
 # 与登录解耦 传递cookie
 def get_kblist(xnm, xqm, cook):
    # 课表查询
    if xqm == '3':
        xqm = '16'  # 第三学期
    if xqm == '1':
        xqm = '3'  # 第一学期
    if xqm == '2':
        xqm = '12'  # 第二学期
    url = 'http://jwxt.cumt.edu.cn/jwglxt/kbcx/xskbcx_cxXsKb.html?gnmkdm=N2151'
    form_data = {
        'xnm': xnm,
        'xqm': xqm,
    }
    headers = {
        'X-Requested-With': 'XMLHttpRequest',
        'Cookie': cook,
    }
    a = requests.post(url=url, data=form_data, headers=headers)
    return a.json()   
# 对json数据进行清洗， 可能会抛异常，外面需要try catch包一下
def marshal_new_kb(data):
    l1 = data['kbList']
    f1 = []
    for l2 in l1:
        d1 = {}
        d1.update({
            "title": l2['kcmc'],
            "location": l2['cdmc'],
            "teacher": l2['xm'],
            "credit": l2['xf'],
            "weekList": get_list(l2['zcd']),
            "weekNum": int(l2['xqj']),
            "lessonNum": int(l2['jcs'].split("-")[0]),
            "durationNum": int(l2['jcs'].split("-")[1]) - int(l2['jcs'].split("-")[0]) + 1,
            "remark": ""
        })
        f1.append(d1)
    return f1


```




### 图书馆
同理 拿到 jwt

- 借阅列表
- 历史借阅列表

部分代码如下
```python
class newIds:
    def __init__(self, username, password):
      。。。。
    def login(self):
      。。。。  
    def get_library_token(self):
        r = self.session.get(url=url_library_re, headers=headers, allow_redirects=False)
        r2 = self.session.get(url=r.headers['Location'], headers=headers, allow_redirects=False)
        r3 = self.session.get(url=r2.headers['Location'], headers=headers, allow_redirects=False)
        r4 = self.session.get(url=r3.headers['Location'], headers=headers, allow_redirects=False)
        # 重定向出现问题，手动跳转获取token, 反正以后也得拆分。。。
        # print('图书馆所用的jwtOpacAuth为：', r4.headers['Location'][43:-12])
        return r4.headers['Location'][43:-12]
 
class libIds:
    def __init__(self, jwt_token):
        self.token = jwt_token
        self.headers = {
            'User-Agent': "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:29.0) Gecko/20100101 FireFox / 29.0",
            "X-Requested-With": "XMLHttpRequest",
            "jwtOpacAuth": jwt_token,
            "Referer": "https://findcumt.libsp.com/",
            "Connection": "close"
        }

    def get_library_list(self, page='1', rows='20'):  # page 页吗 rows 每页行数
        form = {
            "page": page,
            "rows": rows
        }
        r = requests.post(url=url_library_Loan, headers=self.headers, json=form, verify=False)
        return r.json()

    def get_library_history_list(self, page='1', rows='20'):
        form = {
            "page": page,
            "rows": rows
        }
        r = requests.post(url=url_library_loan_history, headers=self.headers, json=form, verify=False)
        return r.json()

    def get_library_favorite(self, page='1', rows='10'):
        form = {
            "favoritesId": "",
            "page": page,
            "rows": rows,
            "searchField": "title",
            "searchFieldContent": ""
        }
        r = requests.post(url=url_library_favorite, headers=self.headers, json=form, verify=False)
        return r.json()
```
### 一卡通
同理拿到 hallticket

- 一卡通充值(不会立即到账，存在过渡余额)
- 一卡通详细流水

部分代码如下
```python
    def get_balance_charge(self, tranamt='100'):
        # 获取Cookie
        r = self.session.get(url=url_balance_re1, headers=headers)
        soup = BeautifulSoup(r.text, 'html5lib')
        token = soup.find('input', id='ssoticketid')['value']
        form = {
            "errorcode": 1,
            "continueurl": '',
            "ssoticketid": token
        }
        r1 = self.session.post(url=url_balance_re2, headers=headers, data=form)
        # 获取卡号
        r2 = self.session.get(url=url_balance, headers=headers)
        account = r2.json()['data']['ZH']

        # 以下为本次表单提交
        header = {
            "Referer": "http://ykt.cumt.edu.cn/Page/Page",
            "Cookie": 'hallticket='+l1[6].value
        }
        form_charge = {
            "account": account,
            "acctype": "23%23%23",
            "tranamt": tranamt,
            "qpwd": "",
            "paymethod": "2",
            "paytype": "%E4%BD%BF%E7%94%A8%E7%BB%91%E5%AE%9A%E7%9A%84%E9%BB%98%E8%AE%A4%E8%B4%A6%E5%8F%B7",
            "client_type": "web"
        }
        r3 = requests.post(url=url_balance_charge, headers=header, data=form_charge)
        return r3.text
```
## 总结
都差不多。。。
爬出一个来就可以都爬了
但是需要注意的是：

- 遵守协议，没事看看最新爬虫坐牢案例(doge)
- 我们爬取的东西并不是一成不变的，比如教务系统一更新，代码直接报废
- 被学校约谈。。。。
